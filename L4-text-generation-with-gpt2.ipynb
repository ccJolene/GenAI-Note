{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experimenting with HuggingFace - Text Generation\n\n*Author: Jolene Chen*\n\n**I have recently decided to explore the ins and outs of the ðŸ˜Š Transformers library and this is the next chapter in that journey. In this notebook, I will explore text generation using a GPT-2 model, which was trained to predict next words on 40GB of Internet text data. The fully trained model is actually not available as the creators were concerned about '[malicious applications of the technology](https://openai.com/blog/better-language-models/)', but there is a much smaller version that is available for enthusiants to play with, which we will use here**\n\n**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way. This project is a work in progress and I will continually update it as I learn more about text generation. Feel free to comment with any questions/suggestions:**\n\n**Update: they just released the GPT-3 model (more [here](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/)) and it has 175 billion parameters and it is shockingly intelligent**","metadata":{}},{"cell_type":"code","source":"#for reproducability\nimport random\nSEED = 42\nrandom.seed(SEED)\n\n#maximum number of words in output text\nMAX_LEN = 70","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:16:58.969745Z","iopub.execute_input":"2025-08-19T01:16:58.970001Z","iopub.status.idle":"2025-08-19T01:16:58.977646Z","shell.execute_reply.started":"2025-08-19T01:16:58.969971Z","shell.execute_reply":"2025-08-19T01:16:58.976363Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"input_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:16:58.978527Z","iopub.execute_input":"2025-08-19T01:16:58.979390Z","iopub.status.idle":"2025-08-19T01:16:58.993514Z","shell.execute_reply.started":"2025-08-19T01:16:58.979352Z","shell.execute_reply":"2025-08-19T01:16:58.992784Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# I. Intro\n\n**A language model is a machine learning model that can look at part of a sentence and predict the next word/sequence of words. Much like the autofill features on your iPhone/Android, GPT-2 is capable of next word prediction on a much larger and more sophisticated scale. For reference, the smallest available GPT-2 has 117 million parameters, whereas the largest one (invisible to the public) has over 1.5 billion parameters. The largest one available for public use is half the size of their main GPT-2 model**\n\n**ðŸ˜Š Transformers makes it very easy to import this model with both PyTorch and TensorFlow - in this notebook we will be using TensorFlow but it is just as easy in PyTorch. Both the model and its Tokenizer can be imported from the `transformers` library that anyone can get by typing `!pip install transformers`. Let's see just how simple it is to generate text with a neural network. We begin with our input sequence:**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n#get large GPT2 tokenizer and GPT2 model\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\nGPT2 = AutoModelForCausalLM.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:16:58.995445Z","iopub.execute_input":"2025-08-19T01:16:58.995658Z","iopub.status.idle":"2025-08-19T01:17:40.851379Z","shell.execute_reply.started":"2025-08-19T01:16:58.995643Z","shell.execute_reply":"2025-08-19T01:17:40.850842Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932a2edb1e1a4d859049784291efa884"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d924fc88902e406d8905f74a90c5ef4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8cfd84249184710a18a204751eec0e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00cae44768143859cb9d8d734a173a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91b9d5dd3ea4c2da0083404b3226c58"}},"metadata":{}},{"name":"stderr","text":"2025-08-19 01:17:14.434399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755566234.628055      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755566234.684540      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe49b227b273496baa249796d03be79f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1b3a03d8b749799f6f66442895ee28"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"#view model parameters\nfrom torchinfo import summary\nsummary(GPT2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:17:40.852028Z","iopub.execute_input":"2025-08-19T01:17:40.852392Z","iopub.status.idle":"2025-08-19T01:17:40.913055Z","shell.execute_reply.started":"2025-08-19T01:17:40.852376Z","shell.execute_reply":"2025-08-19T01:17:40.912443Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"===========================================================================\nLayer (type:depth-idx)                             Param #\n===========================================================================\nGPT2LMHeadModel                                    --\nâ”œâ”€GPT2Model: 1-1                                   --\nâ”‚    â””â”€Embedding: 2-1                              64,328,960\nâ”‚    â””â”€Embedding: 2-2                              1,310,720\nâ”‚    â””â”€Dropout: 2-3                                --\nâ”‚    â””â”€ModuleList: 2-4                             --\nâ”‚    â”‚    â””â”€GPT2Block: 3-1                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-2                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-3                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-4                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-5                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-6                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-7                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-8                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-9                         19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-10                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-11                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-12                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-13                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-14                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-15                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-16                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-17                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-18                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-19                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-20                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-21                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-22                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-23                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-24                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-25                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-26                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-27                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-28                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-29                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-30                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-31                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-32                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-33                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-34                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-35                        19,677,440\nâ”‚    â”‚    â””â”€GPT2Block: 3-36                        19,677,440\nâ”‚    â””â”€LayerNorm: 2-5                              2,560\nâ”œâ”€Linear: 1-2                                      64,328,960\n===========================================================================\nTotal params: 838,359,040\nTrainable params: 838,359,040\nNon-trainable params: 0\n==========================================================================="},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"GPT2.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:17:40.913778Z","iopub.execute_input":"2025-08-19T01:17:40.913996Z","iopub.status.idle":"2025-08-19T01:17:47.302075Z","shell.execute_reply.started":"2025-08-19T01:17:40.913962Z","shell.execute_reply":"2025-08-19T01:17:47.301136Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"GPT2Config {\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 1280,\n  \"n_head\": 20,\n  \"n_inner\": null,\n  \"n_layer\": 36,\n  \"n_positions\": 1024,\n  \"pad_token_id\": 50256,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.52.4\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# II. Different Decoding Methods\n\n## First Pass (Greedy Search)\n\n**With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated via:**\n\n$$w_t = argmax_{w}P(w | w_{1:t-1})$$\n\n**at each timestep $t$. Let's see how this naive approach performs:**","metadata":{}},{"cell_type":"code","source":"# encode context the generation is conditioned on\ninput_ids = tokenizer.encode(input_sequence, return_tensors='pt')\n\n# generate text until the output length (which includes the context length) reaches 50\ngreedy = GPT2.generate(input_ids, max_length=MAX_LEN)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy[0], skip_special_tokens = True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:17:47.303116Z","iopub.execute_input":"2025-08-19T01:17:47.303463Z","iopub.status.idle":"2025-08-19T01:17:57.154458Z","shell.execute_reply.started":"2025-08-19T01:17:47.303424Z","shell.execute_reply":"2025-08-19T01:17:57.153726Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n\nI'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my office.\n\nI'm not talking about the gym that\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"And there we go: generating text is that easy. Our results are not great - as we can see, our model starts repeating itself rather quickly. The main issue with Greedy Search is that words with high probabilities can be masked by words in front of them with low probabilities, so the model is unable to explore more diverse combinations of words. We can prevent this by implementing Beam Search:","metadata":{}},{"cell_type":"markdown","source":"## Beam Search with N-Gram Penalities\n\n**Beam search is essentially Greedy Search but the model tracks and keeps `num_beams` of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting `no_repeat_ngram_size = 2` which ensures that no 2-grams appear twice. We will also set `num_return_sequences = 5` so we can see what the other 5 beams looked like**\n\n**To use Beam Search, we need only modify some parameters in the `generate` function:**","metadata":{}},{"cell_type":"code","source":"# set return_num_sequences > 1\nbeam_outputs = GPT2.generate(\n    input_ids, \n    max_length = MAX_LEN, \n    num_beams = 5, \n    no_repeat_ngram_size = 2, \n    num_return_sequences = 5, \n    early_stopping = True\n)\n\nprint('')\nprint(\"Output:\\n\" + 100 * '-')\n\n# now we have 3 output sequences\nfor i, beam_output in enumerate(beam_outputs):\n      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:17:57.155063Z","iopub.execute_input":"2025-08-19T01:17:57.155334Z","iopub.status.idle":"2025-08-19T01:18:19.404653Z","shell.execute_reply.started":"2025-08-19T01:17:57.155313Z","shell.execute_reply":"2025-08-19T01:18:19.403868Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nOutput:\n----------------------------------------------------------------------------------------------------\n0: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n\n\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I mean, it's\n1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n\n\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a girl\n2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n\n\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a woman\n3: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n\n\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a beautiful\n4: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n\n\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I'm not sure if\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**Now that's much better! The 5 different beam hypotheses are pretty much all the same, but if we increaed `num_beams`, then we would see some more variation in the separate beams. But of course, Beam Search is not perfect either. It works well when the legnth of the generated text is more or less constant, like problems in translation or summarization, but not so much for open-ended problems like dialog or story generation (because it is much harder to find a balance between `num_beams` and `no_repeat_ngram_size`)**\n\n**Furthermore, [research](https://arxiv.org/abs/1904.09751) shows that human languages do not follow this 'high probability word next' distribution. This makes sense - if my words were exactly what you expected them to be, I would be quite a boring person and most people don't want to be boring! The below graph plots the difference of Beam Search and actual human speech: ![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)**\n\nTaken from original paper [here](https://arxiv.org/abs/1904.09751)","metadata":{}},{"cell_type":"markdown","source":"## Basic Sampling\n\n**Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:**\n\n$$w_t \\sim P(w|w_{1:t-1})$$\n\n**However, when we include this randomness, the generated text tends to be incoherent (see more [here](https://arxiv.org/pdf/1904.09751.pdf)) so we can include the `temperature` parameter which increases the chances of high probability words and decreases the chances of low probability words in the sampling:**\n\n**We just need to set `do_sample = True` to implement sampling and for demonstration purposes (you'll shortly see why) we set `top_k = 0`:**","metadata":{}},{"cell_type":"code","source":"# use temperature to decrease the sensitivity to low probability candidates\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 0, \n                             temperature = 0.8\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:18:19.405502Z","iopub.execute_input":"2025-08-19T01:18:19.405798Z","iopub.status.idle":"2025-08-19T01:18:21.618033Z","shell.execute_reply.started":"2025-08-19T01:18:19.405779Z","shell.execute_reply":"2025-08-19T01:18:21.617233Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work: I want to stop and take a walk.\"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Top-K Sampling\n\n**In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together**\n\n**We just need to set `top_k` to however many of the top words we want to consider for our conditional probability distribution:**","metadata":{}},{"cell_type":"code","source":"#sample from only top_k most likely words\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 50\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:18:21.620586Z","iopub.execute_input":"2025-08-19T01:18:21.620821Z","iopub.status.idle":"2025-08-19T01:18:29.897024Z","shell.execute_reply.started":"2025-08-19T01:18:21.620804Z","shell.execute_reply":"2025-08-19T01:18:29.896346Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work and after I've finished up an interview. I want to lie down and sleep. And I want you to take me there. I want you to lay down with me just that one time. You know, after all, that's ...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Top-K Sampling seems to generate more coherent text than our random sampling before. But we can do even better:\n","metadata":{}},{"cell_type":"markdown","source":"## Top-P Sampling\n\n**Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set**\n\n**The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set `top_k = 0` and choose a value `top_p`, where 0 <`top_p`< 1:**","metadata":{}},{"cell_type":"code","source":"#sample only from 80% most likely words\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_p = 0.8, \n                             top_k = 0\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:18:29.897749Z","iopub.execute_input":"2025-08-19T01:18:29.898020Z","iopub.status.idle":"2025-08-19T01:18:38.390616Z","shell.execute_reply.started":"2025-08-19T01:18:29.898002Z","shell.execute_reply":"2025-08-19T01:18:38.389937Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work. I want to do a little chat. If you need help, you can call, or send me a message here:\n\nnacho965@gmail.com\n\nThe Question\n\nWho are the worst offensive linemen ...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Top-K and Top-P Sampling\n\n**As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both `top_k` and `top_p`. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:**","metadata":{}},{"cell_type":"code","source":"#combine both sampling techniques\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .7,\n                              top_k = 50, \n                              top_p = 0.85, \n                              num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:18:38.391271Z","iopub.execute_input":"2025-08-19T01:18:38.391472Z","iopub.status.idle":"2025-08-19T01:19:29.094112Z","shell.execute_reply.started":"2025-08-19T01:18:38.391457Z","shell.execute_reply":"2025-08-19T01:19:29.093188Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\n0: I don't know about you, but there's only one thing I want to do after a long day of work: sit down and read a book.\"\n\nThe first thing you'll notice is the book is not a traditional biography. Instead, this book takes readers into the author's private world, through his experiences in the military. The story starts at the very beginning and follows the journey of two soldiersâ€”one on active duty and one on reserveâ€”who both have similar experiences. Their lives and choices diverge, but in the end, they come to terms with their experiences and are able to move forward.\n\n\"I'm not a great reader,\" says the author. \"When...\n\n1: I don't know about you, but there's only one thing I want to do after a long day of work: make sure everyone is happy!\n\nI don't know if I'll be able to get the photos I wanted for this post, but hopefully this will at least give you some ideas on what to capture and what not to use!\n\nI'm hoping to post more of my photography adventures from around the world soon, and as always, I'll have more from the Philippines soon!\n\nIf you liked the post, please share it using the social media buttons below, and please follow me on Facebook, Instagram, Twitter, and Pinterest!\n\nYou might also like...\n\n2: I don't know about you, but there's only one thing I want to do after a long day of work: drink a beer and watch a football game. You might be able to do that without having to leave your couch.\n\nAdvertisement\n\nSo, if you're still up, you'll find some more details about this in tomorrow's issue of New York magazine....\n\n3: I don't know about you, but there's only one thing I want to do after a long day of work. I want to get a quick snack, go out and do something cool, and just relax. So if you have the spare time in your day, why not go ahead and grab some lunch with us?\n\nI'll be right here in my chair, enjoying my lunch. Just a small one. You want something large? Here you go. I won't bite, so just give me a second.\n\nGood lunch, bye!\n\nOh, and the second part is even more entertaining:\n\nAdvertisements...\n\n4: I don't know about you, but there's only one thing I want to do after a long day of work.\n\nI want to sleep.\n\nYes, that's right.\n\nIf I have a choice, I'm gonna sleep.\n\nIt might sound crazy, but for me it's a very simple truth.\n\nI love sleeping.\n\nI don't want to make my wife wake up every single night in a panic about me not showing up for work.\n\nI don't want to have to deal with the fear that I may be late.\n\nI want to get all of the work done when I get home and then curl up in...\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# III. Benchmark Prompts\n\n**Here, we will see how well the GPT-2 model does when given some more interesting inputs. The following prompts are taken from [OpenAI's GPT2](https://openai.com/blog/better-language-models/) website where they feed them to their full sized GPT2 model (and the results are astounding, I highly recommend you check out their [page](https://openai.com/)**","metadata":{}},{"cell_type":"markdown","source":"## \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"","metadata":{}},{"cell_type":"code","source":"prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n\ninput_ids = tokenizer.encode(prompt1, return_tensors='pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:21:05.128559Z","iopub.execute_input":"2025-08-19T01:21:05.128806Z","iopub.status.idle":"2025-08-19T01:21:05.133274Z","shell.execute_reply.started":"2025-08-19T01:21:05.128788Z","shell.execute_reply":"2025-08-19T01:21:05.132450Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"sample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:21:05.134122Z","iopub.execute_input":"2025-08-19T01:21:05.134371Z","iopub.status.idle":"2025-08-19T01:21:09.922011Z","shell.execute_reply.started":"2025-08-19T01:21:05.134349Z","shell.execute_reply":"2025-08-19T01:21:09.921247Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\n0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\nThey were found in the wild and had been living in the remote valley for more than a century. However, this...\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## \"Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\"\n\n**Can we use GPT-2 to generate fake news stories?**","metadata":{}},{"cell_type":"code","source":"prompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\n\ninput_ids = tokenizer.encode(prompt2, return_tensors='pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:20:55.227700Z","iopub.execute_input":"2025-08-19T01:20:55.227960Z","iopub.status.idle":"2025-08-19T01:20:55.233214Z","shell.execute_reply.started":"2025-08-19T01:20:55.227942Z","shell.execute_reply":"2025-08-19T01:20:55.232411Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"sample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85\n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:20:56.311912Z","iopub.execute_input":"2025-08-19T01:20:56.312152Z","iopub.status.idle":"2025-08-19T01:21:05.127403Z","shell.execute_reply.started":"2025-08-19T01:20:56.312135Z","shell.execute_reply":"2025-08-19T01:21:05.126727Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\n0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n\nThe singer was spotted wearing a dress she bought from the department store, which has been widely mocked for the price tag.\n\nAbercrombie and Fitch, which is owned by department store giant Gap Inc, is known for...\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\"\n\n**Can we use GPT-2 to imagine alternate histories of Lord of the Rings?**","metadata":{}},{"cell_type":"code","source":"prompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n\ninput_ids = tokenizer.encode(prompt3, return_tensors='pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:21:09.923244Z","iopub.execute_input":"2025-08-19T01:21:09.923475Z","iopub.status.idle":"2025-08-19T01:21:09.927762Z","shell.execute_reply.started":"2025-08-19T01:21:09.923459Z","shell.execute_reply":"2025-08-19T01:21:09.927012Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"sample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:21:09.928443Z","iopub.execute_input":"2025-08-19T01:21:09.928614Z","iopub.status.idle":"2025-08-19T01:21:18.952787Z","shell.execute_reply.started":"2025-08-19T01:21:09.928601Z","shell.execute_reply":"2025-08-19T01:21:18.951998Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\n0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. They could only watch as the great army came marching towards them. A battle was raging in the sky, the orcs were battling and the horses were trampling the soldiers to the ground. Gimli and Aragorn had fought well in the past and they...\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## \"For todayâ€™s homework assignment, please describe the reasons for the US Civil War.\"\n\n**Can we use GPT-2 to do our homework?**","metadata":{}},{"cell_type":"code","source":"prompt4 = \"For todayâ€™s homework assignment, please describe the reasons for the US Civil War.\"\n\ninput_ids = tokenizer.encode(prompt4, return_tensors='pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:21:21.193865Z","iopub.execute_input":"2025-08-19T01:21:21.194128Z","iopub.status.idle":"2025-08-19T01:21:21.198254Z","shell.execute_reply.started":"2025-08-19T01:21:21.194109Z","shell.execute_reply":"2025-08-19T01:21:21.197480Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"sample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n    print('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T01:21:21.547255Z","iopub.execute_input":"2025-08-19T01:21:21.547485Z","iopub.status.idle":"2025-08-19T01:21:30.636463Z","shell.execute_reply.started":"2025-08-19T01:21:21.547467Z","shell.execute_reply":"2025-08-19T01:21:30.635732Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\n0: For todayâ€™s homework assignment, please describe the reasons for the US Civil War. If the US was attacked or threatened with being attacked, would you defend the US or not? Would you support the US or not? The following questions are simple, but are likely to provoke some interesting conversations among your peers. Your answer may be used to help...\n\n","output_type":"stream"}],"execution_count":23}]}